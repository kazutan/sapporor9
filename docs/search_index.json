[
["index.html", "データハンドリングFAQ はじめに 参考資料", " データハンドリングFAQ 前田和寛(@kazutan) 2019-03-02 はじめに これはSappoRo.R #9 にて同タイトルで発表した内容です。本編は次のページ以降になります 参考資料 WIP… "],
["01_case_one.html", "1 Case 1: multi-gather/spread問題 1.1 Question 1 1.2 Answer 1.3 解説 1.4 参考資料", " 1 Case 1: multi-gather/spread問題 1.1 Question 1 以下のようなデータがあります: # 資料を読んでる人が再現できるように、データ生成用コードも残しときます n = 30 df_1 &lt;- data.frame( id = rep(1:10, each = 3), group = rep(letters[1:3], 10), x_pre = round(rnorm(n) * 100), x_post = round(rnorm(n, 1, 1) * 100), y_pre = round(rnorm(n, 2, 3) * 100), y_post = round(rnorm(n, 3, 1) * 100) ) knitr::kable(head(df_1)) id group x_pre x_post y_pre y_post 1 a -148 122 -209 326 1 b -54 -45 511 300 1 c -100 253 67 212 2 a -43 39 -336 198 2 b -174 -8 29 328 2 c 149 229 8 318 これを、以下のようにしたいです: id pre_post a_x a_y b_x b_y c_x c_y 1 post 122 326 -45 300 253 212 1 pre -148 -209 -54 511 -100 67 2 post 39 198 -8 328 229 318 2 pre -43 -336 -174 29 149 8 3 post 123 76 180 398 134 219 3 pre -58 -451 148 147 -91 47 どうしたらいいのでしょうか? 1.2 Answer 以下のようにやります: library(tidyverse) df_1_result &lt;-df_1 %&gt;% gather(key = var_name, value = value, -c(id, group)) %&gt;% separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) %&gt;% unite(new_var, group, var) %&gt;% spread(key = new_var, value = value) knitr::kable(head(df_1_result)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 122 326 -45 300 253 212 1 pre -148 -209 -54 511 -100 67 2 post 39 198 -8 328 229 318 2 pre -43 -336 -174 29 149 8 3 post 123 76 180 398 134 219 3 pre -58 -451 148 147 -91 47 1.3 解説 1.3.1 考え方 通称｢multi-gather, multi-spread問題｣の一種です。 wide-long変換をするにはtidyr::gatherやtidyr::spreadを使えばいいのですが、今回は単純にそれらを使うだけではうまく行きません。そこで以下のようなアプローチをします: 一旦tidyなデータ(long data)に整形 変数名を切り離す 目的の変数名を作成 新たに作った変数名をkeyにしてwideに展開 1.3.2 手順 まずはgather: res &lt;- df_1 %&gt;% gather(key = var_name, value = value, -c(id, group)) knitr::kable(head(res)) id group var_name value 1 a x_pre -148 1 b x_pre -54 1 c x_pre -100 2 a x_pre -43 2 b x_pre -174 2 c x_pre 149 ここからがポイントで、当初の変数名を2つに切り離します: res &lt;- res %&gt;% separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) knitr::kable(head(res)) id group var pre_post value 1 a x pre -148 1 b x pre -54 1 c x pre -100 2 a x pre -43 2 b x pre -174 2 c x pre 149 これで要素がちゃんと分かれたデータになりました。そして目的の変数名になるようひっつけます: res &lt;- res %&gt;% unite(new_var, group, var) knitr::kable(head(res)) id new_var pre_post value 1 a_x pre -148 1 b_x pre -54 1 c_x pre -100 2 a_x pre -43 2 b_x pre -174 2 c_x pre 149 あとはこの変数名の列をkeyとしてwideにします: res &lt;- res %&gt;% spread(key = new_var, value = value) knitr::kable(head(res)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 122 326 -45 300 253 212 1 pre -148 -209 -54 511 -100 67 2 post 39 198 -8 328 229 318 2 pre -43 -336 -174 29 149 8 3 post 123 76 180 398 134 219 3 pre -58 -451 148 147 -91 47 これでOKです。 1.3.3 応用 今回はvalueにあたるデータが全て数値だったのでスムーズでしたが、型が違う場合もあります: n = 30 df_1a &lt;- data.frame( id = rep(1:10, each = 3), group = rep(letters[1:3], 10), x_pre = round(rnorm(n) * 100), x_post = round(rnorm(n, 1, 1) * 100), y_pre = sample(c(&quot;kosaki&quot;, &quot;chitoge&quot;), n, replace = TRUE, prob = c(5, 5)), y_post = sample(c(&quot;kosaki&quot;, &quot;chitoge&quot;), n, replace = TRUE, prob = c(9, 1)) ) knitr::kable(head(df_1a)) id group x_pre x_post y_pre y_post 1 a -34 383 chitoge kosaki 1 b 112 150 chitoge kosaki 1 c -70 152 chitoge chitoge 2 a -9 117 kosaki kosaki 2 b 156 191 chitoge kosaki 2 c -82 62 chitoge kosaki この場合、まずは気にせずに同じように整形し、あとから列の型を変更すればOKです res_a &lt;- df_1a %&gt;% # このときvalueがcharacter型になる gather(key = var_name, value = value, -c(id, group)) %&gt;% # 気にせず処理 separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) %&gt;% unite(new_var, group, var) %&gt;% spread(new_var, value) %&gt;% # 数値にしたい列を変換 mutate_at(vars(ends_with(&quot;_x&quot;)), as.numeric) knitr::kable(head(res_a)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 383 kosaki 150 kosaki 152 chitoge 1 pre -34 chitoge 112 chitoge -70 chitoge 2 post 117 kosaki 191 kosaki 62 kosaki 2 pre -9 kosaki 156 chitoge -82 chitoge 3 post -4 kosaki 98 kosaki 21 kosaki 3 pre -117 kosaki 231 kosaki -48 kosaki 以上です。なお個人的には全てtidyにしたいです。 1.4 参考資料 "],
["02_case_two.html", "2 Case 2: ファイル一気読み問題 2.1 Question 2 2.2 Answer 2.3 解説 2.4 参考資料", " 2 Case 2: ファイル一気読み問題 2.1 Question 2 あるディレクトリ内に、一つのファイルに1人分のデータが入ったcsvファイルがたくさんあります: set.seed(57) library(tidyverse) # ティレクトリ作成 if (!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } # CSVファイルを作成 # 中身は確認しといてください tmp_df &lt;- data.frame() for (i in 1:10) { path &lt;- paste(&quot;data/file_&quot;, LETTERS[i], &quot;.csv&quot;) tmp_df &lt;- sample_n(iris, sample(1:4, 1)) write.csv(tmp_df, path, row.names = FALSE) } これを一気にまとめて読み込んで、ひとつのデータにまとめたいです。どうしたら一番ラクでしょうか? また、後から確認できるように、どのファイルから持ってきたデータなのかの情報も加えたいです。 2.2 Answer こんな感じです: # csvが入っているディレクトリからCSVファイル名を取得 csv_fname &lt;- dir(&quot;data&quot;, full.names = TRUE) %&gt;% str_subset(&quot;.csv$&quot;) # 読み込み関数を定義 r_csv_w_fname &lt;- function(path) { if (file.exists(path)) { df &lt;- read_csv(path) if (nrow(df) &gt; 0) { df &lt;- mutate(df, fname = path) return(df) } } } # あとはこの1行でOK df &lt;- map_dfr(csv_fname, r_csv_w_fname) # 内容を確認 knitr::kable(sample_n(df, 10)) Sepal.Length Sepal.Width Petal.Length Petal.Width Species fname 5.4 3.4 1.5 0.4 setosa data/file_ J .csv 6.8 2.8 4.8 1.4 versicolor data/file_ A .csv 5.0 3.4 1.5 0.2 setosa data/file_ J .csv 6.3 2.5 5.0 1.9 virginica data/file_ J .csv 6.7 3.3 5.7 2.1 virginica data/file_ F .csv 5.2 3.4 1.4 0.2 setosa data/file_ E .csv 6.3 2.5 5.0 1.9 virginica data/file_ G .csv 6.7 3.3 5.7 2.5 virginica data/file_ G .csv 5.5 2.4 3.7 1.0 versicolor data/file_ E .csv 4.8 3.4 1.9 0.2 setosa data/file_ B .csv 2.3 解説 2.3.1 考え方 通称｢ファイル一気読み問題｣といわれるものです。今回は読み込んでつなげる上に、｢ファイル名の情報を追加しろ｣といわれています。あとから確認できるようにすることはとても大切です。 基本的な考え方は以下のとおりです: 読み込むファイルのバスを準備 パスにあるファイルを読み込む 読み込んだデータにファイル情報を付与 データを結合 それでは今回の内容について、順を追って説明します。 2.3.2 手順 まずはcsvファイルのパスを準備します: csv_fname &lt;- dir(&quot;data&quot;, full.names = TRUE) %&gt;% str_subset(&quot;.csv$&quot;) やり方は色々あるでしょうが、私はだいいたいこんな感じでやります。 stringr::str_subset() は文字列ベクトルからパターンにマッチした文字列を残します。また、ファイル名だけではパスとして不十分なので、dir関数のfull.names引数でフルパスを取得するようにしています。 ｢読み込んでデータを加工する｣を繰り返すのでforループしかないと思うかもしれませんが、自分で関数を定義して準備するといいでしょう: # 読み込み関数を定義 r_csv_w_fname &lt;- function(path) { if (file.exists(path)) { df &lt;- read_csv(path) if (nrow(df) &gt; 0) { df &lt;- mutate(df, fname = path) return(df) } } } 内容はシンプルなので問題ないかとは思います。なおifをつけなくても今回のは動くのですが、ある程度は対処していた方がいいです。あと、データハンドリングでは自作関数を準備する場面がかなり多いです。 関数を準備したので、あとはこの関数にパスを順次送り込んで、返り値を行方向に結合していけばOKです。Rのbaseにはapplyがありますが、今回は purrr::map_dfrが断然楽です: df &lt;- map_dfr(csv_fname, r_csv_w_fname) map_dfr関数はmap -&gt; as.data.frame -&gt; bind_rows というのを一気にやってくれるイメージです。なお、pam_dfrにはbind_rowsと同じく.id`引数があるので、｢単純にどのファイルからやってきているかさえ識別できればいい｣のであれば、関数を定義せずにこれだけでもいいと思います。 2.3.3 応用 もしファイルがcsvではなくExcelファイルなどである場合は、read_csvではなく他の読み込み関数を使えばOKです。 また、自作関数内で処理を加えれば、いろいろなことができるでしょう。 2.4 参考資料 "],
["04_case_four.html", "3 Case 3: Nested df問題 3.1 Question 3 3.2 Answer 3.3 解説 3.4 参考資料", " 3 Case 3: Nested df問題 3.1 Question 3 以下のようなデータがあります: set.seed(57) library(tidyverse) library(lubridate) library(prophet) start_date = &quot;2019-01-01&quot; n = 100 df_4 &lt;- data.frame( yyyymmdd = seq(date(start_date), date(start_date) + days(n - 1), by = &quot;day&quot;), y_1 = sin(n) * 10 + rnorm(n), y_2 = sin(n) * 3 + rnorm(n, 2, 0.01), y_3 = cos(n) * 8 + rnorm(n) ) knitr::kable(head(df_4)) yyyymmdd y_1 y_2 y_3 2019-01-01 -5.757423 0.4802741 8.615518 2019-01-02 -6.830485 0.4812141 5.996507 2019-01-03 -4.441120 0.4799585 6.123853 2019-01-04 -3.046980 0.4899672 7.284474 2019-01-05 -4.922723 0.4894491 6.583605 2019-01-06 -3.437993 0.4807437 9.632790 このy_*に対して、prophetで予測させて未来のデータもあわせて作成したいです: y_var ds yhat yhat_lower yhat_upper y y_1 2019-01-01 -4.988744 -6.498488 -3.384716 -5.757423 y_1 2019-01-02 -5.378460 -6.842081 -3.815985 -6.830485 y_1 2019-01-03 -5.113240 -6.610575 -3.731660 -4.441120 y_1 2019-01-04 -4.901305 -6.258661 -3.484424 -3.046980 y_1 2019-01-05 -4.663884 -6.151910 -3.336566 -4.922723 y_1 2019-01-06 -4.793852 -6.307509 -3.414466 -3.437993 y_1 2019-01-07 -4.998286 -6.480009 -3.494410 -3.673579 y_1 2019-01-08 -4.993733 -6.425913 -3.536153 -5.941644 y_1 2019-01-09 -5.383449 -6.830365 -4.008951 -6.092223 y_1 2019-01-10 -5.118228 -6.462090 -3.545616 -4.112209 3.2 Answer こんな感じでOKです: m &lt;- function(d) { # fitting model. ここは各自ががんばってください model &lt;- prophet(d) # forecast future &lt;- make_future_dataframe(model, periods = 7) forecast &lt;- predict(model, future) # 欲しい部分を抽出して加工 res &lt;- forecast %&gt;% mutate(ds = date(ds)) %&gt;% select(ds, yhat, yhat_lower, yhat_upper) %&gt;% right_join(d) # return return(res) } df_4_result &lt;- df_4 %&gt;% gather(key = y_var, value = y, -yyyymmdd) %&gt;% rename(ds = yyyymmdd) %&gt;% group_by(y_var) %&gt;% nest() %&gt;% mutate(fit = map(data, m)) %&gt;% select(y_var, fit) %&gt;% unnest() knitr::kable(head(df_4_result, 10)) y_var ds yhat yhat_lower yhat_upper y y_1 2019-01-01 -4.988744 -6.452807 -3.509133 -5.757423 y_1 2019-01-02 -5.378460 -6.912193 -4.018997 -6.830485 y_1 2019-01-03 -5.113240 -6.575139 -3.735088 -4.441120 y_1 2019-01-04 -4.901305 -6.342179 -3.391854 -3.046980 y_1 2019-01-05 -4.663884 -6.120531 -3.264734 -4.922723 y_1 2019-01-06 -4.793852 -6.252291 -3.396149 -3.437993 y_1 2019-01-07 -4.998286 -6.445208 -3.590837 -3.673579 y_1 2019-01-08 -4.993733 -6.497540 -3.644810 -5.941644 y_1 2019-01-09 -5.383449 -6.968824 -3.936416 -6.092223 y_1 2019-01-10 -5.118228 -6.627071 -3.736248 -4.112209 3.3 解説 3.3.1 考え方 典型的なnested-df案件で、purrr::mapが本領発揮するケースです。｢繰り返しやること｣は関数化して、それをpurrr::mapでまとめましょう。 流れは以下のとおりです: modelingと必要な値を抽出する処理を関数化 使うデータセットをtidyに nest化 nest化したデータのそれぞれに準備した関数を当て、その結果を新たな列としてmutate 必要な部分だけ取り出してunnest 3.3.2 手順 まずはモデリングして整形する関数を作成します: m &lt;- function(d) { # fitting model. ここは各自ががんばってください model &lt;- prophet(d) # forecast future &lt;- make_future_dataframe(model, periods = 7) forecast &lt;- predict(model, future) # 欲しい部分を抽出して加工 res &lt;- forecast %&gt;% mutate(ds = date(ds)) %&gt;% select(ds, yhat, yhat_lower, yhat_upper) %&gt;% right_join(d) # return return(res) } このケースではprophetで予測しています。prophet便利ですよね。関数を作成したら、まずはこの関数単体でちゃんと動くか、実際にデータを渡してテストしてください。なお、ここで関数化している理由はいろいろありますが、このようにモデリング処理を関数化することによってモデルを修正していくコストを減らずことが大きいです。実際モデリングは何度も何度も繰り返すので、メンテしやすくしておくのは大切です。 あとはデータフローとなります。nest化を行います: res &lt;- df_4 %&gt;% gather(key = y_var, value = y, -yyyymmdd) %&gt;% rename(ds = yyyymmdd) %&gt;% group_by(y_var) %&gt;% nest() res #&gt; # A tibble: 3 x 2 #&gt; y_var data #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 y_1 &lt;tibble [100 × 2]&gt; #&gt; 2 y_2 &lt;tibble [100 × 2]&gt; #&gt; 3 y_3 &lt;tibble [100 × 2]&gt; group_byしてからnestすることで、データを層別化できます。一部を取り出してみると、こんな感じです: res$data[[2]] #&gt; # A tibble: 100 x 2 #&gt; ds y #&gt; &lt;date&gt; &lt;dbl&gt; #&gt; 1 2019-01-01 0.480 #&gt; 2 2019-01-02 0.481 #&gt; 3 2019-01-03 0.480 #&gt; 4 2019-01-04 0.490 #&gt; 5 2019-01-05 0.489 #&gt; 6 2019-01-06 0.481 #&gt; 7 2019-01-07 0.473 #&gt; 8 2019-01-08 0.485 #&gt; 9 2019-01-09 0.466 #&gt; 10 2019-01-10 0.484 #&gt; # … with 90 more rows あとは｢各行のdataに対してmodeling｣します。ポイントは｢新たな列として、関数処理した結果を追加する｣というイメージです: res &lt;- res %&gt;% mutate(fit = map(data, m)) 今回作成したmという関数は、実測値(y)と予測値(yhat)および予測の上限･下限を含むdata.frameを返します。そのため、以下のような感じになります: res #&gt; # A tibble: 3 x 3 #&gt; y_var data fit #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 y_1 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; #&gt; 2 y_2 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; #&gt; 3 y_3 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; res$fit[[2]] %&gt;% head() #&gt; ds yhat yhat_lower yhat_upper y #&gt; 1 2019-01-01 0.4773502 0.4649810 0.4897357 0.4802741 #&gt; 2 2019-01-02 0.4805435 0.4670079 0.4934888 0.4812141 #&gt; 3 2019-01-03 0.4819156 0.4696508 0.4946583 0.4799585 #&gt; 4 2019-01-04 0.4804376 0.4667566 0.4936465 0.4899672 #&gt; 5 2019-01-05 0.4870225 0.4744456 0.4993775 0.4894491 #&gt; 6 2019-01-06 0.4821182 0.4700115 0.4949034 0.4807437 あとは、これを普段使うdata.frameのような感じに展開したいのですが、これはunnestでOKです: res &lt;- res %&gt;% select(y_var, fit) %&gt;% unnest() 3.3.3 応用 今回のデータをy_varをkeyとして横に展開したい場合もあるでしょうが、これはすでにケース1で紹介しているので省略します。 また多くの場合、これをggplot2などでplotすると思います。これもnest化してやってしまいましょう: # plotする部分を関数化 f_p &lt;- function(df, var_name) { ggplot(df, aes(x = ds)) + # geom geom_ribbon(aes(ymin = yhat_lower, ymax = yhat_upper), color = &quot;#9999FF&quot;, fill = &quot;#9999FF99&quot;) + geom_line(aes(y = yhat), color = &quot;#0000FF&quot;) + geom_point(aes(y = y), color = &quot;#0000AA&quot;) + # scales scale_x_date(date_labels = &quot;%m-%d&quot;) + # theme &amp; labs theme_bw() + labs(title = var_name) } res &lt;- res %&gt;% group_by(y_var) %&gt;% nest() %&gt;% mutate(plot = map2(data, y_var, f_p)) res$plot %&gt;% gridExtra::marrangeGrob(nrow = 2, ncol = 2) 3.4 参考資料 "],
["05_case_five.html", "4 Case 4: window関数問題 4.1 Question 4 4.2 Answer 4.3 解説 4.4 応用 4.5 参考資料", " 4 Case 4: window関数問題 4.1 Question 4 以下のようなログデータがあります: library(tidyverse) library(lubridate) set.seed(57) n = 1000 df_5 &lt;- data.frame( user_id = 100000 + sample(1:50, n, replace = TRUE), shop_id = sample(paste(&quot;shop&quot;, str_pad(1:10, width = 2, pad = &quot;0&quot;), sep = &quot;_&quot;), n, replace = TRUE), dt = ymd_hms(&quot;2019-02-14 00:00:00&quot;) + days(sample(0:27, n, replace = TRUE)) + hours(sample(17:23, n, replace = TRUE)) + minutes(sample(0:59, n, replace = TRUE)) + seconds(sample(0:59, n, replace = TRUE)) ) knitr::kable(head(df_5)) user_id shop_id dt 100013 shop_09 2019-03-12 18:05:34 100026 shop_01 2019-03-01 18:09:00 100002 shop_08 2019-03-07 17:19:27 100009 shop_06 2019-02-23 18:57:06 100037 shop_10 2019-03-06 22:28:48 100034 shop_06 2019-02-14 17:38:08 ここで、shop_id == &quot;shop_05&quot;にはじめて訪問する前と後で、ユーザーが各店舗に何回訪問したかを集計したいです: user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 4.2 Answer いろいろやり方がありますが、こんな感じでもできます: df_5_result &lt;- df_5 %&gt;% arrange(user_id, dt) %&gt;% group_by(user_id) %&gt;% mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) %&gt;% mutate(cum_target = cumsum(target_flag)) %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) %&gt;% group_by(user_id, shop_id, pre_post) %&gt;% summarise(count = n()) %&gt;% spread(pre_post, count, fill = 0) knitr::kable(head(df_5_result)) user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 4.3 解説 4.3.1 考え方 ログデータを前処理するときに頻出するパターンですが、その状況によってバリエーションが出てきます。このケースでは、フラグ立てとwindow関数の応用でいけます。 流れは以下のとおりです: user_id, dtで並べ替え user単位でgroup by ターゲットに訪問したレコード(行)にフラグ(1)を立てる ユーザー単位でフラグの累積和を算出 フラグ累積和が0(つまりまだターゲットに訪問していない)のレコードをpre、それ以外をpostとする あとはgroup byし直して普通に集計 ポイントは3-5です。SQLのノウハウでもよく共有されてるやり方をdplyrでトレースしてます。 4.3.2 手順 まずは並べ替えてgroup by: res &lt;- df_5 %&gt;% arrange(user_id, dt) %&gt;% group_by(user_id) knitr::kable(head(res)) user_id shop_id dt 100001 shop_06 2019-02-14 21:04:51 100001 shop_05 2019-02-14 22:52:52 100001 shop_05 2019-02-16 22:08:02 100001 shop_03 2019-02-17 19:45:45 100001 shop_07 2019-02-19 18:02:35 100001 shop_04 2019-02-20 20:50:28 今回の処理では、常にユーザー単位で処理をします。また、時系列で並べ替えておく必要もあるのでこの操作となります。 次に、ターゲットに訪問しているログを特定します: res &lt;- res %&gt;% mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag 100001 shop_06 2019-02-14 21:04:51 0 100001 shop_05 2019-02-14 22:52:52 1 100001 shop_05 2019-02-16 22:08:02 1 100001 shop_03 2019-02-17 19:45:45 0 100001 shop_07 2019-02-19 18:02:35 0 100001 shop_04 2019-02-20 20:50:28 0 100001 shop_10 2019-02-22 19:41:16 0 100001 shop_03 2019-02-22 23:51:35 0 100001 shop_01 2019-02-24 20:27:48 0 100001 shop_09 2019-02-25 19:16:26 0 100001 shop_06 2019-02-26 21:50:13 0 100001 shop_01 2019-02-26 22:43:49 0 100001 shop_06 2019-02-27 21:23:47 0 100001 shop_09 2019-02-28 19:05:35 0 100001 shop_10 2019-03-04 19:23:41 0 100001 shop_05 2019-03-04 23:53:19 1 100001 shop_07 2019-03-04 23:53:47 0 100001 shop_05 2019-03-05 17:13:47 1 100001 shop_04 2019-03-06 20:44:25 0 100001 shop_04 2019-03-06 23:01:55 0 この後に、累積和を算出するdplyr::cumsumを利用します: res &lt;- res %&gt;% mutate(cum_target = cumsum(target_flag)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag cum_target 100001 shop_06 2019-02-14 21:04:51 0 0 100001 shop_05 2019-02-14 22:52:52 1 1 100001 shop_05 2019-02-16 22:08:02 1 2 100001 shop_03 2019-02-17 19:45:45 0 2 100001 shop_07 2019-02-19 18:02:35 0 2 100001 shop_04 2019-02-20 20:50:28 0 2 100001 shop_10 2019-02-22 19:41:16 0 2 100001 shop_03 2019-02-22 23:51:35 0 2 100001 shop_01 2019-02-24 20:27:48 0 2 100001 shop_09 2019-02-25 19:16:26 0 2 100001 shop_06 2019-02-26 21:50:13 0 2 100001 shop_01 2019-02-26 22:43:49 0 2 100001 shop_06 2019-02-27 21:23:47 0 2 100001 shop_09 2019-02-28 19:05:35 0 2 100001 shop_10 2019-03-04 19:23:41 0 2 100001 shop_05 2019-03-04 23:53:19 1 3 100001 shop_07 2019-03-04 23:53:47 0 3 100001 shop_05 2019-03-05 17:13:47 1 4 100001 shop_04 2019-03-06 20:44:25 0 4 100001 shop_04 2019-03-06 23:01:55 0 4 この結果を見ればすぐにわかるかと思います。cumsumとかはwindow関数と呼ばれ、mutateの中で活用します。SQLだとover句をイメージしてもらえるとスムーズです。 このとき、0となっているのは｢まだフラグが立ってない時期(つまり訪問前)のログ｣となりますので、あとはこれを利用してpre-postラベルを準備します: res &lt;- res %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag cum_target pre_post 100001 shop_06 2019-02-14 21:04:51 0 0 pre 100001 shop_05 2019-02-14 22:52:52 1 1 post 100001 shop_05 2019-02-16 22:08:02 1 2 post 100001 shop_03 2019-02-17 19:45:45 0 2 post 100001 shop_07 2019-02-19 18:02:35 0 2 post 100001 shop_04 2019-02-20 20:50:28 0 2 post 100001 shop_10 2019-02-22 19:41:16 0 2 post 100001 shop_03 2019-02-22 23:51:35 0 2 post 100001 shop_01 2019-02-24 20:27:48 0 2 post 100001 shop_09 2019-02-25 19:16:26 0 2 post 100001 shop_06 2019-02-26 21:50:13 0 2 post 100001 shop_01 2019-02-26 22:43:49 0 2 post 100001 shop_06 2019-02-27 21:23:47 0 2 post 100001 shop_09 2019-02-28 19:05:35 0 2 post 100001 shop_10 2019-03-04 19:23:41 0 2 post 100001 shop_05 2019-03-04 23:53:19 1 3 post 100001 shop_07 2019-03-04 23:53:47 0 3 post 100001 shop_05 2019-03-05 17:13:47 1 4 post 100001 shop_04 2019-03-06 20:44:25 0 4 post 100001 shop_04 2019-03-06 23:01:55 0 4 post ここまでくれば、あとは集計です: res &lt;- res %&gt;% group_by(user_id, shop_id, pre_post) %&gt;% summarise(count = n()) knitr::kable(head(res, 10)) user_id shop_id pre_post count 100001 shop_01 post 5 100001 shop_02 post 2 100001 shop_03 post 2 100001 shop_04 post 3 100001 shop_05 post 5 100001 shop_06 post 2 100001 shop_06 pre 1 100001 shop_07 post 2 100001 shop_08 post 1 100001 shop_09 post 2 さて、ここでログデータお約束の｢ログがないデータは集計できない｣問題が発生します。例えば、｢1回目からshop_05に来たならば、その人のpreログデータは生成されていないので出てこない｣といった状況です。 この対処法はいくつかあるのですが、今回は面倒だったのでspreadするときにfill = 0として埋めることにしました: res &lt;- res %&gt;% spread(pre_post, count, fill = 0) knitr::kable(head(res, 10)) user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 100001 shop_07 2 0 100001 shop_08 1 0 100001 shop_09 2 0 100001 shop_10 2 0 これで完了です。 4.4 応用 今回のケースはズバリそのものというシチュエーションは少ないですが、これをベースにいろんな応用ができます。たとえば、｢shop_05への初回訪問以降、ある店を訪問した後に、次に訪問した店を把握できるようなデータがほしい｣という場合には、以下のようになります: df_5_fromto &lt;- df_5 %&gt;% # 並べ替えとユーザー単位でグループ化 arrange(user_id, dt) %&gt;% group_by(user_id) %&gt;% # フラグ立てとpre-postラベル付与 mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) %&gt;% mutate(cum_target = cumsum(target_flag)) %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) %&gt;% # lead関数でひとつ下へずらす mutate(from_shop_id = shop_id, to_shop_id = lead(shop_id)) %&gt;% # preはいらんので取り除く filter(pre_post != &quot;pre&quot;) knitr::kable(head(df_5_fromto, 20)) user_id shop_id dt target_flag cum_target pre_post from_shop_id to_shop_id 100001 shop_05 2019-02-14 22:52:52 1 1 post shop_05 shop_05 100001 shop_05 2019-02-16 22:08:02 1 2 post shop_05 shop_03 100001 shop_03 2019-02-17 19:45:45 0 2 post shop_03 shop_07 100001 shop_07 2019-02-19 18:02:35 0 2 post shop_07 shop_04 100001 shop_04 2019-02-20 20:50:28 0 2 post shop_04 shop_10 100001 shop_10 2019-02-22 19:41:16 0 2 post shop_10 shop_03 100001 shop_03 2019-02-22 23:51:35 0 2 post shop_03 shop_01 100001 shop_01 2019-02-24 20:27:48 0 2 post shop_01 shop_09 100001 shop_09 2019-02-25 19:16:26 0 2 post shop_09 shop_06 100001 shop_06 2019-02-26 21:50:13 0 2 post shop_06 shop_01 100001 shop_01 2019-02-26 22:43:49 0 2 post shop_01 shop_06 100001 shop_06 2019-02-27 21:23:47 0 2 post shop_06 shop_09 100001 shop_09 2019-02-28 19:05:35 0 2 post shop_09 shop_10 100001 shop_10 2019-03-04 19:23:41 0 2 post shop_10 shop_05 100001 shop_05 2019-03-04 23:53:19 1 3 post shop_05 shop_07 100001 shop_07 2019-03-04 23:53:47 0 3 post shop_07 shop_05 100001 shop_05 2019-03-05 17:13:47 1 4 post shop_05 shop_04 100001 shop_04 2019-03-06 20:44:25 0 4 post shop_04 shop_04 100001 shop_04 2019-03-06 23:01:55 0 4 post shop_04 shop_01 100001 shop_01 2019-03-06 23:30:15 0 4 post shop_01 shop_02 あとは集計したりネットワーク分析をしたりと利活用できるでしょう。 4.5 参考資料 "]
]
